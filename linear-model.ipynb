{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法的缺陷在于如果特征有相关性，矩阵会趋向奇异，计算的偏性会很大\n",
    "衡量值有：Mean squared error、variance\n",
    "算法复杂度：如果特征矩阵（n、p），O（np2）\n",
    "### 预测boston house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "boston=load_boston()\n",
    "reg=linear_model.LinearRegression()\n",
    "scores=cross_val_score(reg,boston['data'],boston['target'],scoring='r2',cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression 岭回归\n",
    "增加了alpha w2的[正则项](http://www.cnblogs.com/jianxinzhou/p/4083921.html)\n",
    "alpha就是regularization（正则参数）\n",
    "当没有最够多的数据时，使用多次项进行拟合，易过拟合\n",
    "处理过拟合方法：\n",
    "一、减少特征\n",
    "二、保留所有特征，增加正则项，减少单独特征的影响\n",
    "因为高次项特征会被放大，所以需要用一个penalty（惩罚项）去平衡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       \n",
    "reg.alpha_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = linear_model.RidgeCV(alphas=[0.01,0.1,1.0,10.0,100.0])\n",
    "scores=cross_val_score(reg,boston['data'],boston['target'],scoring='r2',cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lasso regression\n",
    "Lasso回归与岭回归的不同之处在于其在惩罚函数中使用了绝对值而非平方和的形式，这将导致在模型的参数估计过程中有些系数会因为惩罚项的存在从而直接减少到0。随着惩罚力度的增强，越来越多的系数将会缩小并最终归结为0，这意味着在模型构建的同时我们也对原本给定的多个变量进行了变量选择\n",
    "- 这种回归在模型假设方面与岭回归完全一致\n",
    "- 模型可将某些变量对应的系数直接收缩至0，这有助于变量的筛选(feature selection)\n",
    "- 该模型属于L1惩罚\n",
    "- 如果存在某组预测因子高度相关的情况，Lasso方法仅会选取它们中的一个，并把所对应的系数收缩至0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ElasticNet \n",
    "可以说，弹性网回归是对Lasso回归与岭回归的一种折中，在其模型训练时对L1和L2两种正则化方法进行了综合。当数据集中的许多特征都呈现出相互关联的情况时，弹性网就可以大展身手了，与Lasso相比，它不再随机选择单个变量，而是倾向于把他们全部揪出来。\n",
    "- 当存在多个相互关联的变量时，弹性网回归将加强它们之间的组效应（group effect）\n",
    "- 该模型对选定变量的个数没有任何的限制\n",
    "- 该模型能够承受双重的收缩"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
